{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n",
      "['i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film']\n",
      "['on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its']\n",
      "['at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her']\n",
      "['just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good']\n",
      "['more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time']\n",
      "['only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me']\n",
      "['than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also']\n",
      "['other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont']\n",
      "['made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n",
      "['after', 'characters', 'think', 'watch', 'many', 'two', 'seen', 'character', 'being', 'never']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, preprocessing, optimizers, losses, metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path =  \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000   # 仅考虑最高频的 10000 个词\n",
    "MAX_LEN = 200       # 每个样本保留 200 个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "# 构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line, \"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\n",
    "    text = tf.expand_dims(arr[1], axis=0)\n",
    "    return (text, label)\n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames=[train_data_path]) \\\n",
    "   .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames=[test_data_path]) \\\n",
    "   .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# 构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation), '')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split='whitespace',\n",
    "    max_tokens=MAX_WORDS-1,  # 有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "# print(vectorize_layer.get_vocabulary()[0:100])\n",
    "for idx in range(11):\n",
    "    print(vectorize_layer.get_vocabulary()[idx * 10:(idx + 1) * 10])\n",
    "\n",
    "\n",
    "# 单词编码\n",
    "ds_train = ds_train_raw.map(lambda text, label:(vectorize_layer(text), label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text, label:(vectorize_layer(text), label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 7)            70000     \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              (None, 196, 16)           576       \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling1D)        (None, 98, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv1D)              (None, 97, 128)           4224      \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling1D)        (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 6145      \n",
      "=================================================================\n",
      "Total params: 80,945\n",
      "Trainable params: 80,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 演示自定义模型范例，实际上应该优先使用 Sequential 或者函数式 API\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size= 5, name=\"conv_1\", activation=\"relu\")\n",
    "        self.pool_1 = layers.MaxPool1D(name=\"pool_1\")\n",
    "        self.conv_2 = layers.Conv1D(128, kernel_size=2, name=\"conv_2\", activation=\"relu\")\n",
    "        self.pool_2 = layers.MaxPool1D(name=\"pool_2\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1, activation=\"sigmoid\")\n",
    "        super(CnnModel, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n",
    "    \n",
    "    # 用于显示Output Shape\n",
    "    def summary(self):\n",
    "        x_input = layers.Input(shape=MAX_LEN, name=\"input\")\n",
    "        output = self.call(x_input)\n",
    "        model = tf.keras.Model(inputs = x_input, outputs = output, name=\"model\")\n",
    "        model.summary()\n",
    "\n",
    "model = CnnModel()\n",
    "model.build(input_shape =(None, MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    today_ts = tf.timestamp() % (24 * 60 * 60)\n",
    "    \n",
    "    hour = tf.cast(today_ts // 3600 + 8, tf.int32) % tf.constant(24)\n",
    "    minite = tf.cast((today_ts % 3600) // 60, tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts % 60), tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m)) == 1:\n",
    "            return(tf.strings.format(\"0{}\", m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\", m))\n",
    "\n",
    "    timestring = tf.strings.join([timeformat(hour), timeformat(minite),\n",
    "                timeformat(second)], separator=\":\")\n",
    "    tf.print(\"==========\" * 9 + timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================11:47:47\n",
      "Epoch=1, Loss:0.170583129, Accuracy:0.93625, Valid Loss:0.361590534, Valid Accuracy:0.8712\n",
      "\n",
      "==========================================================================================11:47:58\n",
      "Epoch=2, Loss:0.113764673, Accuracy:0.95955, Valid Loss:0.439444721, Valid Accuracy:0.8708\n",
      "\n",
      "==========================================================================================11:48:09\n",
      "Epoch=3, Loss:0.0647901, Accuracy:0.9784, Valid Loss:0.602329791, Valid Accuracy:0.8648\n",
      "\n",
      "==========================================================================================11:48:19\n",
      "Epoch=4, Loss:0.0350493118, Accuracy:0.9885, Valid Loss:0.800360799, Valid Accuracy:0.86\n",
      "\n",
      "==========================================================================================11:48:28\n",
      "Epoch=5, Loss:0.0180068184, Accuracy:0.9945, Valid Loss:0.981879413, Valid Accuracy:0.859\n",
      "\n",
      "==========================================================================================11:48:38\n",
      "Epoch=6, Loss:0.0143510057, Accuracy:0.9955, Valid Loss:1.15362406, Valid Accuracy:0.854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features, training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features, training=False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    for epoch in tf.range(1, epochs + 1):\n",
    "        \n",
    "        for features, labels in ds_train:\n",
    "            train_step(model, features, labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model, features, labels)\n",
    "        \n",
    "        # 此处 logs 模板需要根据 metric 具体情况修改\n",
    "        logs = 'Epoch={}, Loss:{}, Accuracy:{}, Valid Loss:{}, Valid Accuracy:{}' \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            printbar()\n",
    "            tf.print(\n",
    "                tf.strings.format(\n",
    "                    logs,\n",
    "                    (\n",
    "                        epoch, \n",
    "                        train_loss.result(),\n",
    "                        train_metric.result(),\n",
    "                        valid_loss.result(),\n",
    "                        valid_metric.result()\n",
    "                    )\n",
    "                ))\n",
    "            tf.print(\"\")\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "train_model(model, ds_train, ds_test, epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "         valid_step(model, features,labels)\n",
    "    logs = 'Valid Loss: {}, Valid Accuracy: {}' \n",
    "    tf.print(tf.strings.format(logs, (valid_loss.result(), valid_metric.result())))\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.15362406, Valid Accuracy: 0.854\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6412138 ],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       ...,\n",
       "       [0.996657  ],\n",
       "       [0.99167955],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.4121389e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.6712733e-08]\n",
      " [9.9923491e-01]\n",
      " [1.7198188e-08]\n",
      " [1.0513377e-09]\n",
      " [2.0263493e-03]\n",
      " [9.9999881e-01]\n",
      " [9.9999964e-01]\n",
      " [1.0000000e+00]\n",
      " [9.9939692e-01]\n",
      " [3.8780005e-08]\n",
      " [9.9943322e-01]\n",
      " [2.7051968e-07]\n",
      " [9.7854507e-01]\n",
      " [1.8463733e-07]\n",
      " [6.7190182e-01]\n",
      " [8.2147717e-03]\n",
      " [9.9381411e-01]], shape=(20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x_test, _ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    # 以下方法等价\n",
    "    # print(model.call(x_test))\n",
    "    # print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/FDUHYJ/anaconda3/envs/lisa/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/FDUHYJ/anaconda3/envs/lisa/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./data/tf_model_savedmodel/assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6412138 ],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       ...,\n",
       "       [0.996657  ],\n",
       "       [0.99167955],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\n",
    "model_loaded.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
